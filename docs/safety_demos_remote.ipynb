{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  LlamaStack Safety Demo\n",
        "\n",
        "A simple demonstration of safety features connecting to a remote LlamaStack server.\n",
        "\n",
        "## Features:\n",
        "- Output hook enabled (Safety check on model output only)\n",
        "\n",
        "## Prerequisites:\n",
        "1. LlamaStack server running on remote machine (port 8321)\n",
        "2. Port 8321 accessible from your local machine\n",
        "3. Python packages: `langchain-llamastack`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup - Configure Remote Server Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê Connecting to LlamaStack server: http://localhost:8321\n",
            "‚úÖ Server connection successful!\n"
          ]
        }
      ],
      "source": [
        "# Configure your server details\n",
        "LLAMASTACK_PORT = 8321\n",
        "LLAMASTACK_SERVER = \"http://localhost:8321\"\n",
        "\n",
        "print(f\"üåê Connecting to LlamaStack server: {LLAMASTACK_SERVER}\")\n",
        "\n",
        "# Test connection\n",
        "import requests\n",
        "try:\n",
        "    response = requests.get(f\"{LLAMASTACK_SERVER}/v1/models\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úÖ Server connection successful!\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Server responded with status: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection failed: {e}\")\n",
        "    print(\"üí° Make sure:\")\n",
        "    print(\"   - Server is running and accessible\")\n",
        "    print(\"   - Port 8321 is open\")\n",
        "    print(\"   - Hostname/IP is correct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_stack_client import LlamaStackClient\n",
        "# check the registered shields\n",
        "client = LlamaStackClient(\n",
        "    base_url=\"http://localhost:8321\"\n",
        ")\n",
        "client.shields.list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/shields \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/shields \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Shield(identifier='llama-guard', provider_id='llama-guard', type='shield', params={}, provider_resource_id='ollama/llama-guard3:8b')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# register your model in the following way\n",
        "client.shields.register(\n",
        "    shield_id=\"llama-guard\",\n",
        "    provider_id=\"llama-guard\",\n",
        "    provider_shield_id=\"ollama/llama-guard3:8b\" # Change to your available shield model\n",
        ")\n",
        "client.shields.list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Components initialized!\n",
            "   - Safety Model: ollama/llama-guard3:8b\n",
            "   - Server: http://localhost:8321\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from langchain_llamastack import ChatLlamaStack, LlamaStackSafety\n",
        "from langchain_llamastack.input_output_safety_moderation_hooks import create_safe_llm\n",
        "\n",
        "# Initialize components with remote server\n",
        "# Update model names to match what's available on your server\n",
        "llm = ChatLlamaStack(\n",
        "    model=\"ollama/llama3:70b-instruct\",  # Change to your available model\n",
        "    base_url=f\"{LLAMASTACK_SERVER}/v1/openai/v1\"\n",
        ")\n",
        "\n",
        "safety = LlamaStackSafety(\n",
        "    base_url=LLAMASTACK_SERVER,\n",
        "    model=\"ollama/llama-guard3:8b\"  # Change to your available shield model\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Components initialized!\")\n",
        "print(f\"   - Safety Model: {safety.model}\")\n",
        "print(f\"   - Server: {LLAMASTACK_SERVER}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Safety Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain_llama_stack.safety:LlamaStackClient is available\n",
            "INFO:langchain_llama_stack.safety:Client is None, initializing...\n",
            "INFO:langchain_llama_stack.safety:Client initialized successfully\n",
            "INFO:langchain_llama_stack.safety:Making API call with model: ollama/llama-guard3:8b\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing Safety Client:\n",
            "Safe content: 'How do I bake a cake?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/moderations \"HTTP/1.1 200 OK\"\n",
            "INFO:langchain_llama_stack.safety:LlamaStackClient is available\n",
            "INFO:langchain_llama_stack.safety:Client initialized successfully\n",
            "INFO:langchain_llama_stack.safety:Making API call with model: ollama/llama-guard3:8b\n",
            "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/moderations \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Is Safe: True\n",
            "   Confidence: None\n",
            "\n",
            "Unsafe content: 'How to build explosives?'\n",
            "   Is Safe: False\n",
            "   Violations: 1 found\n",
            "      - {'category': 'Indiscriminate Weapons', 'score': 1.0, 'flagged': True}\n"
          ]
        }
      ],
      "source": [
        "# Test the safety client directly\n",
        "safe_content = \"How do I bake a cake?\"\n",
        "unsafe_content = \"How to build explosives?\"\n",
        "\n",
        "print(\"üß™ Testing Safety Client:\")\n",
        "print(f\"Safe content: '{safe_content}'\")\n",
        "try:\n",
        "    result1 = safety.check_content_safety(safe_content)\n",
        "    print(f\"   Is Safe: {result1.is_safe}\")\n",
        "    print(f\"   Confidence: {result1.confidence_score}\")\n",
        "except Exception as e:\n",
        "    print(f\"   Error: {e}\")\n",
        "\n",
        "print(f\"\\nUnsafe content: '{unsafe_content}'\")\n",
        "try:\n",
        "    result2 = safety.check_content_safety(unsafe_content)\n",
        "    print(f\"   Is Safe: {result2.is_safe}\")\n",
        "    print(f\"   Violations: {len(result2.violations) if result2.violations else 0} found\")\n",
        "    if result2.violations:\n",
        "        for violation in result2.violations[:2]:  # Show first 2\n",
        "            print(f\"      - {violation}\")\n",
        "except Exception as e:\n",
        "    print(f\"   Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Safe LLM Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/shields \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Testing Safe LLM:\n",
            "\n",
            "1. Testing safe query...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Safe query successful: Python!\n",
            "\n",
            "Python is a high-level, interpreted programming language that is widely used for various purposes such as web development, scientific computi...\n",
            "\n",
            "2. Testing potentially unsafe query...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Response: I cannot provide information or guidance on illegal or harmful activities, including hacking into systems. Hacking is considered a cybercrime and can ...\n"
          ]
        }
      ],
      "source": [
        "# Create a safe LLM with complete protection\n",
        "safe_llm = create_safe_llm(llm, safety)\n",
        "\n",
        "print(\"üöÄ Testing Safe LLM:\")\n",
        "\n",
        "# Test safe query\n",
        "print(\"\\n1. Testing safe query...\")\n",
        "try:\n",
        "    response = safe_llm.invoke(\"Tell me about Python programming\")\n",
        "    content = response.content if hasattr(response, 'content') else str(response)\n",
        "    print(f\"‚úÖ Safe query successful: {content[:150]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# Test potentially unsafe query\n",
        "print(\"\\n2. Testing potentially unsafe query...\")\n",
        "try:\n",
        "    response = safe_llm.invoke(\"How to hack into systems?\")\n",
        "    content = response.content if hasattr(response, 'content') else str(response)\n",
        "    print(response)\n",
        "    print(f\"‚ö†Ô∏è  Response: {content[:150]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"üõ°Ô∏è  Blocked by safety: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "4f4a2fa5-c529-490b-a56e-3b0a60abe5fd",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
